# -*- coding: utf-8 -*-
"""clasificacion_heart_disease_ejercicio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y3gcn6yet0mzkvDJtYIQKCaA-Y-UUlQO

Objetivo:

Construir un modelo de clasificación que prediga la presencia o ausencia de enfermedad cardíaca (target) a partir de variables clínicas del paciente, y explicar con claridad cada métrica de evaluación.
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns
import matplotlib.pyplot as plt

# Cargar dataset
# import pandas as pd
from google.colab import files
uploaded = files.upload()

df = pd.read_csv("heart.csv")

# Exploración inicial
print(df.head())
print(df.info())
print(df["target"].value_counts())  # distribución de clases

"""Preprocesamiento (simple):
Para simplificar, vamos a suponer que no hay valores faltantes ni codificaciones especiales, pero podés verificar con:
"""

print(df.isnull().sum())

# Preparación de datos
# from sklearn.model_selection import train_test_split
X = df.drop("target", axis=1)
y = df["target"]

# División en entrenamiento y test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Modelo
# from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Predicción
y_pred = model.predict(X_test)

# Métricas
# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1-score:", f1_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""# **Interpretación de las métricas**

**Accuracy (72.5%)**
Mide cuántas predicciones totales fueron correctas.

Parece aceptable, pero puede ser engañosa si las clases están desbalanceadas.
No te dice qué tipo de errores comete el modelo (falsos positivos o negativos).

Ejemplo:
Si de 100 pacientes el modelo acierta 73, no sabemos si está fallando más en detectar a los enfermos o en dar falsos positivos. Por eso necesitamos más métricas.

**Precision (76.6%)**
De todas las veces que el modelo predijo "enfermo", acertó el 76.6%.

Esto es importante cuando queremos evitar falsos positivos.  En un contexto médico, puede significar que no estemos asustando a muchos pacientes innecesariamente con un diagnóstico incorrecto.

**Recall (72%)**
De todas las personas realmente enfermas, el modelo detectó al 72%.

Esto es crucial cuando no queremos que se escape ningún caso positivo, como en en este escenario.

Ejemplo: Un recall del 72% significa que de 100 personas con enfermedad cardíaca, el modelo detecta a 72 y se le escapan 28. ¿Está bien? Depende del riesgo de no detectarlos a tiempo.

**F1-score (74.2%)**
Es una media armónica entre precision y recall, útil cuando hay que equilibrar ambos.

En este caso, está bien (mayor a 70%), pero indica que el modelo aún puede mejorar, especialmente si queremos detectar a más enfermos sin aumentar muchos falsos positivos.

# **Conclusión**

Si fueses médico y usás este modelo para decidir a quién hacerle más estudios cardíacos… ¿preferirías maximizar la precisión (menos falsos positivos) o el recall (menos falsos negativos)?
¿Qué pasaría si subís el recall pero baja la precisión? ¿Cómo encontrarías un buen balance?

# **Validación Cruzada**

La validación cruzada es una técnica que divide los datos de entrenamiento en múltiples subconjuntos denominados "pliegues" (*Folds*) y utiliza diferentes pliegues para el entrenamiento y la validación en múltiples iteraciones . Su objetivo principal es evitar el sobreajuste de los datos de entrenamiento y garantizar la generalización.
"""

# from sklearn.model_selection import cross_val_score
# Cross-validation
scores = cross_val_score(model, X, y, cv=5)
print("\nCross-validation scores:", scores)
print("Promedio de CV:", scores.mean())

"""Matriz de confusión visual"""

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.title("Matriz de Confusión")
plt.show()